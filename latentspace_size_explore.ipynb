{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will do PCA and maybe some subset selection in this file to determine the size of the latent space in the autoencoder mechanism in USAD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as stats\n",
    "from scipy.stats import pearsonr, spearmanr, mode\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"path to csv\")\n",
    "train = pd.read_csv(\"path to csv\")\n",
    "#combine the csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([test, train], ignore_index=True)\n",
    "df = df.sort_values(by='Timestamp')\n",
    "\n",
    "X = df.sample(n=50, random_state=42)\n",
    "y = X[\"Normal/Attack\"]\n",
    "X = X.drop([\"Timestamp\" , \"Normal/Attack\", \"PASS_ID\"], axis = 1)\n",
    "\n",
    "\n",
    "df = df.drop([\"Timestamp\" , \"Normal/Attack\", \"PASS_ID\" ] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts = len(df) - df.isna().sum()\n",
    "na_df = pd.DataFrame({'Column Name': na_counts.index, 'Number of Non NAs': na_counts.values})\n",
    "\n",
    "display(na_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal = StandardScaler().fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "principalComponents = pca.fit_transform(df_normal)\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Find the number of components for 90% variance\n",
    "n_over_90 = np.argmax(explained_variance >= 0.95) + 1 # \"+1\" because np.argmax returns zero-based indices\n",
    "print(f\"Number of components to explain 90% of the variance: {n_over_90}\")\n",
    "\n",
    "# Plot cumulative explained variance to visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(explained_variance)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_by_components = [(i+1, v) for i, v in enumerate(explained_variance)]\n",
    "for item in explained_variance_by_components:\n",
    "    print(f\"Number of components: {item[0]}, Total explained variance: {item[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also do some recursive feature elimintation with l1 penalty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', penalty = \"l1\", max_iter = 10000)\n",
    "rfe = RFE(model)\n",
    "fit = rfe.fit(X, y)\n",
    "\n",
    "print(\"Number of features selected by subset selection to explain most of the variance: \")\n",
    "print(sum(fit.support_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def forward_selection(X, y):\n",
    "    remaining_features = list(X.columns)\n",
    "    selected_features = []\n",
    "    while remaining_features:\n",
    "        scores = []\n",
    "        for feature in remaining_features:\n",
    "            temp_features = selected_features + [feature]\n",
    "            model = LogisticRegression(max_iter=10000)\n",
    "            score = cross_val_score(model, X[temp_features], y, cv=5).mean()\n",
    "            scores.append((score, feature))\n",
    "        scores.sort()  # ascending order, so the best score is the last one\n",
    "        best_score, best_feature = scores[-1]\n",
    "        remaining_features.remove(best_feature)\n",
    "        selected_features.append(best_feature)\n",
    "        print(f'Added feature {best_feature} with score {best_score}')\n",
    "    return selected_features\n",
    "\n",
    "selected_features = forward_selection(X, y)\n",
    "print(f\"Number of features selected by forward subset selection: {len(selected_features)}\")\n",
    "print(f\"Selected features: {selected_features}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
